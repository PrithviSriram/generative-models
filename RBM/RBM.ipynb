{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gzip, struct\n",
    "\n",
    "class DataSet:\n",
    "    batch_index = 0\n",
    "    \n",
    "    def __init__(self, data_dir, batch_size = None, one_hot = False, seed = 0):\n",
    "        self.data_dir = data_dir\n",
    "        X, Y = self.read()\n",
    "        shape = X.shape\n",
    "        X = X.reshape([shape[0], shape[1] * shape[2]])\n",
    "        self.X = X.astype(np.float)/255\n",
    "        self.size = self.X.shape[0]\n",
    "        if batch_size == None:\n",
    "            self.batch_size = self.size\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        # abandom last few samples\n",
    "        self.batch_num = int(self.size / self.batch_size)\n",
    "        # shuffle samples\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.X)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(Y)\n",
    "        self.one_hot = one_hot\n",
    "        if one_hot:\n",
    "            y_vec = np.zeros((len(Y), 10), dtype=np.float)\n",
    "            for i, label in enumerate(Y):\n",
    "                y_vec[i, Y[i]] = 1.0\n",
    "            self.Y = y_vec\n",
    "        else:\n",
    "            self.Y = Y\n",
    "    \n",
    "    def read(self):\n",
    "        with gzip.open(self.data_dir['Y']) as flbl:\n",
    "            magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "            label = np.fromstring(flbl.read(), dtype=np.int8)\n",
    "        with gzip.open(self.data_dir['X'], 'rb') as fimg:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "            image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "        return image,label\n",
    "    \n",
    "    def next_batch(self):\n",
    "        start = self.batch_index * self.batch_size\n",
    "        end = (self.batch_index + 1) * self.batch_size\n",
    "        self.batch_index = (self.batch_index + 1) % self.batch_num\n",
    "        if self.one_hot:\n",
    "            return self.X[start:end, :], self.Y[start:end, :]\n",
    "        else:\n",
    "            return self.X[start:end, :], self.Y[start:end]\n",
    "        \n",
    "    def sample_batch(self):\n",
    "        index = random.randrange(self.batch_num)\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        if self.one_hot:\n",
    "            return self.X[start:end, :], self.Y[start:end, :]\n",
    "        else:\n",
    "            return self.X[start:end, :], self.Y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_DataSet():\n",
    "    train_dir = {\n",
    "        'X': './mnist/train-images-idx3-ubyte.gz', \n",
    "        'Y': './mnist/train-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    test_dir = {\n",
    "        'X': './mnist/t10k-images-idx3-ubyte.gz', \n",
    "        'Y': './mnist/t10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    mnist = DataSet(test_dir, 2)\n",
    "    print('batch index: %d' % mnist.batch_index)\n",
    "    X, Y = mnist.next_batch()\n",
    "    print('batch index: %d' % mnist.batch_index)\n",
    "    print('X:')\n",
    "    print(X)\n",
    "    print('Y:')\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index: 0\n",
      "batch index: 1\n",
      "X:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Y:\n",
      "[8 7]\n"
     ]
    }
   ],
   "source": [
    "test_DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = {\n",
    "    'X': './mnist/train-images-idx3-ubyte.gz', \n",
    "    'Y': './mnist/train-labels-idx1-ubyte.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dir = {\n",
    "    'X': './mnist/t10k-images-idx3-ubyte.gz', \n",
    "    'Y': './mnist/t10k-labels-idx1-ubyte.gz'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(shape, name='weights'):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias(shape, name='biases'):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape), name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    i = 0 # fliping index for computing pseudo likelihood\n",
    "    \n",
    "    def __init__(self, n_visible=784, n_hidden=500, k=30, momentum=False):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.k = k\n",
    "        \n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        if momentum:\n",
    "            self.momentum = tf.placeholder(tf.float32)\n",
    "        else:\n",
    "            self.momentum = 0.0\n",
    "        self.w = weight([n_visible, n_hidden], 'w')\n",
    "        self.hb = bias([n_hidden], 'hb')\n",
    "        self.vb = bias([n_visible], 'vb')\n",
    "        \n",
    "        self.w_v = tf.Variable(tf.zeros([n_visible, n_hidden]), dtype=tf.float32)\n",
    "        self.hb_v = tf.Variable(tf.zeros([n_hidden]), dtype=tf.float32)\n",
    "        self.vb_v = tf.Variable(tf.zeros([n_visible]), dtype=tf.float32)\n",
    "        \n",
    "    def propup(self, visible):\n",
    "        pre_sigmoid_activation = tf.matmul(visible, self.w) + self.hb\n",
    "        return tf.nn.sigmoid(pre_sigmoid_activation)\n",
    "    \n",
    "    def propdown(self, hidden):\n",
    "        pre_sigmoid_activation = tf.matmul(hidden, tf.transpose(self.w)) + self.vb\n",
    "        return tf.nn.sigmoid(pre_sigmoid_activation)\n",
    "    \n",
    "    def sample_h_given_v(self, v_sample):\n",
    "        h_props = self.propup(v_sample)\n",
    "        h_sample = tf.nn.relu(tf.sign(h_props - tf.random_uniform(tf.shape(h_props))))\n",
    "        return h_sample\n",
    "    \n",
    "    def sample_v_given_h(self, h_sample):\n",
    "        v_props = self.propdown(h_sample)\n",
    "        v_sample = tf.nn.relu(tf.sign(v_props - tf.random_uniform(tf.shape(v_props))))\n",
    "        return v_sample\n",
    "    \n",
    "    def CD_k(self, visibles):       \n",
    "        # k steps gibbs sampling\n",
    "        v_samples = visibles\n",
    "        h_samples = self.sample_h_given_v(v_samples)\n",
    "        for i in range(self.k):\n",
    "            v_samples = self.sample_v_given_h(h_samples)\n",
    "            h_samples = self.sample_h_given_v(v_samples)\n",
    "        \n",
    "        h0_props = self.propup(visibles)\n",
    "        w_positive_grad = tf.matmul(tf.transpose(visibles), h0_props)\n",
    "        w_negative_grad = tf.matmul(tf.transpose(v_samples), h_samples)\n",
    "        w_grad = (w_positive_grad - w_negative_grad) / tf.to_float(tf.shape(visibles)[0])\n",
    "        hb_grad = tf.reduce_mean(h0_props - h_samples, 0)\n",
    "        vb_grad = tf.reduce_mean(visibles - v_samples, 0)\n",
    "        return w_grad, hb_grad, vb_grad\n",
    "    \n",
    "    def learn(self, visibles):\n",
    "        w_grad, hb_grad, vb_grad = self.CD_k(visibles)\n",
    "        # compute new velocities\n",
    "        new_w_v = self.momentum * self.w_v + self.lr * w_grad\n",
    "        new_hb_v = self.momentum * self.hb_v + self.lr * hb_grad\n",
    "        new_vb_v = self.momentum * self.vb_v + self.lr * vb_grad\n",
    "        # update parameters\n",
    "        update_w = tf.assign(self.w, self.w + new_w_v)\n",
    "        update_hb = tf.assign(self.hb, self.hb + new_hb_v)\n",
    "        update_vb = tf.assign(self.vb, self.vb + new_vb_v)\n",
    "        # update velocities\n",
    "        update_w_v = tf.assign(self.w_v, new_w_v)\n",
    "        update_hb_v = tf.assign(self.hb_v, new_hb_v)\n",
    "        update_vb_v = tf.assign(self.vb_v, new_vb_v)\n",
    "        \n",
    "        return [update_w, update_hb, update_vb, update_w_v, update_hb_v, update_vb_v]\n",
    "        \n",
    "    def sampler(self, visibles, steps=5000):\n",
    "        v_samples = visibles\n",
    "        for step in range(steps):\n",
    "            v_samples = self.sample_v_given_h(self.sample_h_given_v(v_samples))\n",
    "        return v_samples\n",
    "    \n",
    "    def free_energy(self, visibles):\n",
    "        first_term = tf.matmul(visibles, tf.reshape(self.vb, [tf.shape(self.vb)[0], 1]))\n",
    "        second_term = tf.reduce_sum(tf.log(1 + tf.exp(self.hb + tf.matmul(visibles, self.w))), axis=1)\n",
    "        return - first_term - second_term\n",
    "    \n",
    "    def pseudo_likelihood(self, visibles):\n",
    "        x = tf.round(visibles)\n",
    "        x_fe = self.free_energy(x)\n",
    "        split0, split1, split2 = tf.split(x, [self.i, 1, tf.shape(x)[1] - self.i - 1], 1)\n",
    "        xi = tf.concat([split0, 1 - split1, split2], 1)\n",
    "        self.i = (self.i + 1) % self.n_visible\n",
    "        xi_fe = self.free_energy(xi)\n",
    "        return tf.reduce_mean(self.n_visible * tf.log(tf.nn.sigmoid(xi_fe - x_fe)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](markov_chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "# 保存图片\n",
    "def save_images(images, size, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Save the samples images\n",
    "    The best size number is\n",
    "            int(max(sqrt(image.shape[0]),sqrt(image.shape[1]))) + 1\n",
    "    example:\n",
    "        The batch_size is 64, then the size is recommended [8, 8]\n",
    "        The batch_size is 32, then the size is recommended [6, 6]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 图片归一化，主要用于生成器输出是tanh形式的归一化\n",
    "    img = (images + 1.0) / 2.0\n",
    "    h, w = img.shape[1], img.shape[2]\n",
    "    \n",
    "    # 生成一个大画布，用来保存生成的batch_size个图像\n",
    "    merge_img = np.zeros((h * size[0], w * size[1]))\n",
    "    \n",
    "    # 循环把画布各个位置的值赋为batch里各幅图像的值\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        merge_img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "        \n",
    "    # 保存画布\n",
    "    return scipy.misc.imsave(path, merge_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_data, epoches):\n",
    "    logs_dir = './logs'\n",
    "    samples_dir = './samples'\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    noise_x, _ = train_data.sample_batch()\n",
    "    # noise_x = tf.random_normal([train_data.batch_size, 784])\n",
    "    rbm = RBM()\n",
    "    step = rbm.learn(x)\n",
    "    sampler = rbm.sampler(x)\n",
    "    pl = rbm.pseudo_likelihood(x)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        mean_cost = []\n",
    "        epoch = 1\n",
    "        for i in range(epoches * train_data.batch_num):\n",
    "            # draw samples\n",
    "            if i % 500 == 0:\n",
    "                samples = sess.run(sampler, feed_dict = {x: noise_x})\n",
    "                samples = samples.reshape([train_data.batch_size, 28, 28])\n",
    "                save_images(samples, [8, 8], os.path.join(samples_dir, 'iteration_%d.png' % i))\n",
    "                print('Saved samples.')\n",
    "            batch_x, _ = train_data.next_batch()\n",
    "            sess.run(step, feed_dict = {x: batch_x, rbm.lr: 0.1})\n",
    "            cost = sess.run(pl, feed_dict = {x: batch_x})\n",
    "            mean_cost.append(cost)\n",
    "            # save model\n",
    "            if i is not 0 and train_data.batch_index is 0:\n",
    "                checkpoint_path = os.path.join(logs_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step = epoch + 1)\n",
    "                print('Saved Model.')\n",
    "            # print pseudo likelihood\n",
    "            if i is not 0 and train_data.batch_index is 0:\n",
    "                print('Epoch %d Cost %g' % (epoch, np.mean(mean_cost)))\n",
    "                mean_cost = []\n",
    "                epoch += 1\n",
    "        print('Test')\n",
    "        samples = sess.run(sampler, feed_dict = {x: noise_x})\n",
    "        samples = samples.reshape([train_data.batch_size, 28, 28])\n",
    "        save_images(samples, [8, 8], os.path.join(samples_dir, 'test.png'))\n",
    "        print('Saved samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = DataSet(data_dir=train_dir, batch_size=64, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 1 Cost -1.92544\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 2 Cost -0.341011\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 3 Cost -0.216929\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 4 Cost -0.148479\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 5 Cost -0.121412\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 6 Cost -0.106842\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 7 Cost -0.0951367\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 8 Cost -0.0763526\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 9 Cost -0.0658732\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 10 Cost -0.0554306\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 11 Cost -0.0514627\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 12 Cost -0.0426717\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 13 Cost -0.0426232\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 14 Cost -0.0357415\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 15 Cost -0.0306901\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 16 Cost -0.0303034\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 17 Cost -0.0287244\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 18 Cost -0.026467\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 19 Cost -0.0265002\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 20 Cost -0.0246736\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 21 Cost -0.0245663\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 22 Cost -0.0233069\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 23 Cost -0.0231338\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 24 Cost -0.0217186\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 25 Cost -0.02157\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 26 Cost -0.0223295\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 27 Cost -0.0200051\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 28 Cost -0.0198551\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 29 Cost -0.0190403\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 30 Cost -0.0199595\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 31 Cost -0.0188231\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 32 Cost -0.0185056\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 33 Cost -0.0203698\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 34 Cost -0.0184213\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 35 Cost -0.0202638\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 36 Cost -0.0209948\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 37 Cost -0.0208861\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 38 Cost -0.0202893\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 39 Cost -0.0190969\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 40 Cost -0.019271\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 41 Cost -0.0187907\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 42 Cost -0.018562\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 43 Cost -0.0193592\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 44 Cost -0.0193858\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 45 Cost -0.0187073\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 46 Cost -0.0181222\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 47 Cost -0.0178379\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 48 Cost -0.0162261\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 49 Cost -0.0160542\n",
      "Saved samples.\n",
      "Saved samples.\n",
      "Saved Model.\n",
      "Epoch 50 Cost -0.0153786\n",
      "Test\n",
      "Saved samples.\n"
     ]
    }
   ],
   "source": [
    "train(train_data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [GitHub: tensorflow-rbm](https://github.com/meownoid/tensorfow-rbm)\n",
    "2. [Theano: RBM](http://deeplearning.net/tutorial/rbm.html#rbm)\n",
    "3. [Stackoverflow: RBM implementation](http://stackoverflow.com/questions/34760981/rbm-implementation-with-tensorflow/35446666#35446666)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
